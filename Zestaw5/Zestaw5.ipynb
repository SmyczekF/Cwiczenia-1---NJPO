{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61c0c9a",
   "metadata": {},
   "source": [
    "Zad.1\n",
    "Porównaj zapisywanie i odczytywanie kolekcji (100, 10000, 100 000 elementów) za pomocą trzech technik: modułu pickle, parquet i xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee4ec64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas operacji zapisu dla kolekcji o długości 100 w module pickle: ok. 0.0 s\n",
      "Czas operacji odczytu dla kolekcji o długości 100 w module pickle: ok. 0.0 s\n",
      "Czas operacji zapisu dla kolekcji o długości 10000 w module pickle: ok. 0.0010027885 s\n",
      "Czas operacji odczytu dla kolekcji o długości 10000 w module pickle: ok. 0.0009965897 s\n",
      "Czas operacji zapisu dla kolekcji o długości 100000 w module pickle: ok. 0.0141186714 s\n",
      "Czas operacji odczytu dla kolekcji o długości 100000 w module pickle: ok. 0.0156621933 s\n",
      "Czas operacji zapisu dla kolekcji o długości 100 w module parquet: ok. 0.0868506432 s\n",
      "Czas operacji odczytu dla kolekcji o długości 100 w module parquet: ok. 0.0 s\n",
      "Czas operacji zapisu dla kolekcji o długości 10000 w module parquet: ok. 8.3829042912 s\n",
      "Czas operacji odczytu dla kolekcji o długości 10000 w module parquet: ok. 0.1428835392 s\n",
      "Czas operacji zapisu dla kolekcji o długości 100000 w module parquet: ok. 79.3076577187 s\n",
      "Czas operacji odczytu dla kolekcji o długości 100000 w module parquet: ok. 1.781264782 s\n",
      "Czas operacji zapisu dla kolekcji o długości 100 w module xlsx: ok. 0.0100669861 s\n",
      "Czas operacji odczytu dla kolekcji o długości 100 w module xlsx: ok. 0.0020442009 s\n",
      "Czas operacji zapisu dla kolekcji o długości 10000 w module xlsx: ok. 0.3739562035 s\n",
      "Czas operacji odczytu dla kolekcji o długości 10000 w module xlsx: ok. 0.505525589 s\n",
      "\n",
      "Wystąpił błąd podczas zapisu pliku 100000 elementowego: Invalid column index 100001!\n",
      "Wiadomość Invalid column index 100001 oznacza, że nie można zapisać pliku o tak dużej ilości kolumn\n",
      "\n",
      " Podsumowanie:\n",
      "\n",
      "Pickle [odczyt szybszy niż zapis]\n",
      "Możemy zauważyć, że najkrótsze czasy odczytu i zapisu słowników 100, 10000 i 100000 elementowych\n",
      "występują dla modułu pickle, jest to związane z faktem, że ów modół zapisuje mniej skomplikowane dane do pliku (słownik),\n",
      "lecz jest najszybszy. Oczywiście nie umożliwia nam on zapisywania struktur bardziej złożonych jak: dataframe lub plik \".xlsx\".\n",
      "\n",
      "Parquet [odczyt znacząco szybszy niż zapis]\n",
      "Kolejny moduł - parquet umożliwia nam zapis i odczyt struktury dataframe, zapis zajmuje zdecydowanie dłużej czasu od pickle i \n",
      "xlsx, lecz jest bardzo wygodny do działań na obiektach dataframe. Umożliwia nam również kompresje pliku.\n",
      "\n",
      "Xlsx [zapis szybszy niż odczyt]\n",
      "Ostatni moduł - xlsx pozwala na zapis i odczyt plików \".xslx\", jest bardzo szybki w swoich działaniach, lecz nie pozwala nam\n",
      "zapisywać ogromnych danych ze względu na ograniczenie kolumn w pliku \".xslx\" - Invalid column index 100001.\n",
      "\n",
      "Każdy z powyższych modułów jest użyteczny na inny sposób i służy do innych czynności.\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import pandas as pd\n",
    "from pickle import dump, load\n",
    "from tempfile import mktemp\n",
    "from os import unlink\n",
    "import time\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "\n",
    "keys100 = list(range(0,100))\n",
    "for i in range(len(keys100)):\n",
    "    keys100[i] = f'element{i}'\n",
    "    \n",
    "keys10000 = list(range(0,10000))\n",
    "for i in range(len(keys10000)):\n",
    "    keys10000[i] = f'element{i}'\n",
    "    \n",
    "keys100000 = list(range(0,100000))\n",
    "for i in range(len(keys100000)):\n",
    "    keys100000[i] = f'element{i}'\n",
    "\n",
    "dict100 = dict.fromkeys(keys100, list(range(1,3)))\n",
    "dict10000 = dict.fromkeys(keys10000, list(range(1,3)))\n",
    "dict100000 = dict.fromkeys(keys100000, list(range(1,3)))\n",
    "def Save_Load_Pickle(collection, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        start = time.time()\n",
    "        dump(collection, f)\n",
    "        end = time.time()\n",
    "\n",
    "        print(f'Czas operacji zapisu dla kolekcji o długości {len(collection)} w module pickle: ok. {round(end - start, 10)} s')\n",
    "\n",
    "        f.flush()\n",
    "\n",
    "    with open(file, 'rb') as f:\n",
    "        sumOf = 0\n",
    "        start = time.time()\n",
    "        temp_col = load(f)\n",
    "        end = time.time()\n",
    "\n",
    "        print(f'Czas operacji odczytu dla kolekcji o długości {len(collection)} w module pickle: ok. {round(end - start, 10)} s')\n",
    "\n",
    "temp_file = mktemp()\n",
    "Save_Load_Pickle(dict100, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "temp_file = mktemp()\n",
    "Save_Load_Pickle(dict10000, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "temp_file = mktemp()\n",
    "Save_Load_Pickle(dict100000, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "def Save_Load_Parquet(collection, file):\n",
    "    df = pd.DataFrame.from_dict(collection)\n",
    "    \n",
    "    start = time.time()\n",
    "    df.to_parquet(file, compression='GZIP')\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Czas operacji zapisu dla kolekcji o długości {len(collection)} w module parquet: ok. {round(end - start, 10)} s')\n",
    "\n",
    "    start = time.time()\n",
    "    pf = ParquetFile(file)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Czas operacji odczytu dla kolekcji o długości {len(collection)} w module parquet: ok. {round(end - start, 10)} s')\n",
    "\n",
    "\n",
    "temp_file = mktemp()\n",
    "Save_Load_Parquet(dict100, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "temp_file = mktemp()\n",
    "Save_Load_Parquet(dict10000, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "temp_file = mktemp()\n",
    "Save_Load_Parquet(dict100000, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "def Save_Load_Xlsx(collection, file):\n",
    "    try:\n",
    "        df = pd.DataFrame.from_dict(collection)\n",
    "        with open(file, 'wb') as f:\n",
    "            wb = Workbook()\n",
    "            ws = wb.active\n",
    "\n",
    "            for r in dataframe_to_rows(df, index=True, header=True):\n",
    "                ws.append(r)\n",
    "\n",
    "            for cell in ws['A'] + ws[1]:\n",
    "                cell.style = 'Pandas'\n",
    "\n",
    "            start = time.time()\n",
    "            wb.save(f.name)\n",
    "            end = time.time()\n",
    "\n",
    "            print(f'Czas operacji zapisu dla kolekcji o długości {len(collection)} w module xlsx: ok. {round(end - start, 10)} s')\n",
    "\n",
    "            f.flush()\n",
    "\n",
    "        with open(file, 'rb') as f:\n",
    "            start = time.time()\n",
    "            wb = load_workbook(f.name)\n",
    "            end = time.time()\n",
    "\n",
    "            print(f'Czas operacji odczytu dla kolekcji o długości {len(collection)} w module xlsx: ok. {round(end - start, 10)} s')\n",
    "    except Exception as e:\n",
    "        print(f'\\nWystąpił błąd podczas zapisu pliku {len(collection)} elementowego: {e}!')\n",
    "        print(f'Wiadomość {e} oznacza, że nie można zapisać pliku o tak dużej ilości kolumn')\n",
    "        \n",
    "temp_file = mktemp(suffix='.xlsx')   \n",
    "Save_Load_Xlsx(dict100, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "temp_file = mktemp(suffix='.xlsx')   \n",
    "Save_Load_Xlsx(dict10000, temp_file)\n",
    "unlink(temp_file)\n",
    "\n",
    "temp_file = mktemp(suffix='.xlsx')   \n",
    "Save_Load_Xlsx(dict100000, temp_file)\n",
    "unlink(temp_file)\n",
    "    \n",
    "\n",
    "print('\\n Podsumowanie:\\n')\n",
    "print('Pickle [odczyt szybszy niż zapis]\\nMożemy zauważyć, że najkrótsze czasy odczytu i zapisu słowników 100, 10000 i 100000 elementowych\\nwystępują dla modułu pickle, jest to związane z faktem, że ów modół zapisuje mniej skomplikowane dane do pliku (słownik),\\nlecz jest najszybszy. Oczywiście nie umożliwia nam on zapisywania struktur bardziej złożonych jak: dataframe lub plik \\\".xlsx\\\".\\n\\nParquet [odczyt znacząco szybszy niż zapis]\\nKolejny moduł - parquet umożliwia nam zapis i odczyt struktury dataframe, zapis zajmuje zdecydowanie dłużej czasu od pickle i \\nxlsx, lecz jest bardzo wygodny do działań na obiektach dataframe. Umożliwia nam również kompresje pliku.\\n\\nXlsx [zapis szybszy niż odczyt]\\nOstatni moduł - xlsx pozwala na zapis i odczyt plików \\\".xslx\\\", jest bardzo szybki w swoich działaniach, lecz nie pozwala nam\\nzapisywać ogromnych danych ze względu na ograniczenie kolumn w pliku \\\".xslx\\\" - Invalid column index 100001.\\n\\nKażdy z powyższych modułów jest użyteczny na inny sposób i służy do innych czynności.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912e949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
